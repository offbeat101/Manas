{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QDP3UIVgxuz",
        "outputId": "33639baa-749a-4028-d0ff-a9a7afd61a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0     subreddit                                               text\n",
            "0           0  suicidewatch  there s point continue lose job last friday I ...\n",
            "1           1  suicidewatch  friend keep find reddit account hate friend mu...\n",
            "2           2  suicidewatch  tired throwaway account I ve marry year think ...\n",
            "3           3  suicidewatch  think mom might commit suicide I â€™ve talk mom ...\n",
            "4           4  suicidewatch  do hii know anyone read I m write this I m wal...\n",
            "subreddit\n",
            "depression      26241\n",
            "suicidewatch    25944\n",
            "casual          25874\n",
            "anxiety         25870\n",
            "mentalhealth    25325\n",
            "lonely          23635\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/preprocessed_data.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "print(data.head())\n",
        "print(data['subreddit'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "data['text_length'] = data['text'].apply(len)\n",
        "data['sentiment'] = data['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "tfidf_features = tfidf.fit_transform(data['text'])\n",
        "\n",
        "features = np.hstack((tfidf_features.toarray(), data[['text_length', 'sentiment']].values))\n",
        "\n",
        "labels = data['subreddit'].factorize()[0]\n",
        "\n",
        "print(features.shape)\n",
        "print(labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzzAHgqlhKPs",
        "outputId": "36ef58ff-2a1e-481d-c4bc-0912585916fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(152889, 502)\n",
            "(152889,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(35,), max_iter=200, random_state=42)\n",
        "\n",
        "\n",
        "model = Pipeline([\n",
        "    ('poly', poly),\n",
        "    ('mlp', mlp)\n",
        "])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "gu7DqfVRhK-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data['subreddit'].unique(), yticklabels=data['subreddit'].unique())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w5pSfhY4hNB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "IhLbWtDahb2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}